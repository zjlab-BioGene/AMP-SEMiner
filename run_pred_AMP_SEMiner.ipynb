{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMP Discovery v1.0\n",
    "\n",
    "The model weights of AMP-SEMiner and minimum datasets to run AMP-SEMiner are available in [Zenodo](https://zenodo.org/records/14348290) (DOI: 10.5281/zenodo.14348290). A small example dataset are also provided:\n",
    "\n",
    "[Tok_CLS.tar.gz](https://zenodo.org/records/14348290/files/Tok_CLS.tar.gz)\n",
    "\n",
    "[Tok_CLS_LoRA.tar.gz](https://zenodo.org/records/14348290/files/Tok_CLS_LoRA.tar.gz)\n",
    "\n",
    "[2_steps.tar.gz](https://zenodo.org/records/14348290/files/2_steps.tar.gz)\n",
    "\n",
    "[example_dataset.tar.gz](https://zenodo.org/records/14348290/files/example_dataset.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.01 setup **Environment** (~8m 28s)\n",
    "%%time\n",
    "import os, time, signal\n",
    "import sys, random, string, re\n",
    "\n",
    "## download model weights and example data\n",
    "!wget https://zenodo.org/records/14348290/files/Tok_CLS.tar.gz\n",
    "!tar -zxvf Tok_CLS.tar.gz\n",
    "\n",
    "!wget https://zenodo.org/records/14348290/files/example_dataset.tar.gz\n",
    "!tar -zxvf example_dataset.tar.gz\n",
    "!head -n 100 example_dataset/APD_dataset.csv > example_data.csv\n",
    "\n",
    "!rm Tok_CLS.tar.gz example_dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import EsmForTokenClassification\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATASET_TRAINING_KEYS = ['input_ids', 'attention_mask']\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_table):\n",
    "        df = pd.read_csv(data_table,header=None)\n",
    "        df.columns = ['Class','ProId','Sequence']\n",
    "        self.names = df['ProId'].tolist()\n",
    "        self.sequences = df['Sequence'].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        sequence = self.sequences[index]\n",
    "        label = torch.from_numpy(np.pad(np.array([0]*len(sequence)),\n",
    "                                 (1,1), mode='constant', constant_values=-100))\n",
    "        return name, label, sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, inputs, names, sequences):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.names = names\n",
    "        self.sequences = sequences\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'ids': idx, 'names': self.names[idx], 'sequences': self.sequences[idx]}\n",
    "\n",
    "## prepare model\n",
    "def get_model(model_name):\n",
    "    print('Loading model from: %s' % model_name)\n",
    "    model = EsmForTokenClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_dataset(input_csv, tokenizer, max_len, batch_size):\n",
    "    print('Loading data from: %s' % input_csv)\n",
    "    val_set = MyDataset(input_csv)\n",
    "    sequences, names = val_set.sequences, val_set.names\n",
    "    inputs = tokenizer(sequences, padding=True, truncation=True, max_length=max_len, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "    eval_dataset = SequenceDataset(inputs, names, sequences)\n",
    "    dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def eval_data(dataloader, model):\n",
    "    print('Predicting...')\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    predicts = {}\n",
    "    proteins = {}\n",
    "    for _, batch in enumerate(dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        names = batch['names']\n",
    "        sequences = batch['sequences']\n",
    "\n",
    "        ins = {k: v for k, v in batch.items() if k in DATASET_TRAINING_KEYS}\n",
    "        ins['input_ids'] = ins['input_ids'].to(device)\n",
    "        ins['attention_mask'] = ins['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(**ins)\n",
    "        logits = outputs.get(\"logits\").detach().cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            seqlen = len(sequences[i])\n",
    "            pred = logits[i][1:seqlen+1].argmax(dim=1)\n",
    "            if pred.nonzero().size(0) >= 5:\n",
    "                proteins[names[i]] = sequences[i]\n",
    "                predicts[names[i]] = pred\n",
    "\n",
    "    return predicts, proteins\n",
    "\n",
    "def get_blocks(pred):\n",
    "    tags, starts, ends = [], [], []\n",
    "    for i in range(pred.shape[0]):\n",
    "        if (i==0) or (pred[i-1] != pred[i]):\n",
    "            tags.append(int(pred[i]))\n",
    "            starts.append(i)\n",
    "        if (i==pred.shape[0]-1) or (pred[i+1] != pred[i]):\n",
    "            ends.append(i)\n",
    "    return torch.tensor(tags), starts, ends\n",
    "\n",
    "def merge_predictions(predicts, proteins):\n",
    "    predictions = {}\n",
    "    for k in predicts.keys():\n",
    "        predictions[k] = {}\n",
    "        tags, starts, ends = get_blocks(predicts[k])\n",
    "        for i in range(len(tags)):\n",
    "            if tags[i] == 1:\n",
    "                predictions[k][proteins[k][starts[i]:ends[i]+1]] = str(starts[i])+','+str(ends[i])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.02 run **AMP Prediction** (~19.2s)\n",
    "%%time\n",
    "\n",
    "model_name = 'Tok_CLS/epoch15' #@param {type:\"string\"}\n",
    "input_data = 'example_data.csv' #@param {type:\"string\"}\n",
    "max_len = 300 #@param [\"300\"] {type:\"raw\"}\n",
    "batch_size = 2 #@param [\"2\"] {type:\"raw\"}\n",
    "output_tab = 'out_pred.tsv' #@param {type:\"string\"}\n",
    "\n",
    "model, tokenizer = get_model(model_name)\n",
    "dataloader = prepare_dataset(input_data, tokenizer, max_len, batch_size)\n",
    "\n",
    "## run prediction\n",
    "predicts, proteins = eval_data(dataloader, model)\n",
    "predictions = merge_predictions(predicts, proteins)\n",
    "\n",
    "## output prediction\n",
    "print('Result output to %s...' % output_tab)\n",
    "with open(output_tab,'w') as f:\n",
    "    f.write('\\t'.join(['ProID','AMP','AMPlen','Position','Sequence'])+'\\n')\n",
    "    for k in predictions.keys():\n",
    "        for a in predictions[k].keys():\n",
    "            f.write('\\t'.join([k, a, str(len(a)),predictions[k][a], proteins[k]])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show predictions\n",
    "results = pd.read_csv(output_tab, sep='\\t')\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
